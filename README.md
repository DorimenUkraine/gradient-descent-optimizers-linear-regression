# linear_regression_optimizers
Gradient descent optimizers for linear regression.

Here are the implemented algorithms :
* Vanilla gradient descent
* Momentum and batch
* Adagrad
* RMSProp
* Adam
* Adamax
* Nesterov Accelerated Gradient
* Nadam

##### TODO:
* Fix Adadelta optimizer
