Theory: https://compsciclub.ru/media/courses/2018-autumn/spb-recommendation/materials/lecture02-linregr_1.pdf

# gradient-descent-optimizers-linear-regression
Gradient descent optimizers for linear regression.

Here are the implemented algorithms :
* Vanilla gradient descent
* Momentum and batch
* Adagrad
* RMSProp
* Adam
* Adamax
* Nesterov Accelerated Gradient
* Nadam

It compares the regression functions and the error evolution.

##### TODO:
* Fix Adadelta optimizer
